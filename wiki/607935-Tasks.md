## Introduction

The NLP Sandbox aims to provide a cloud-based environment that enables the
systematic and continuous benchmarking of NLP Tools that solve specific Tasks.

The first series of NLP Tasks targeted by the NLP Sandbox are the annotation and
de-identification of Protected Health Information (PHI) in clinical notes. This
first series of tasks have been identified through our collaboration with the
first NLP Sandbox Driver, the [National Center for Date to Health (CD2H)].

However, we are counting on more Drivers to collaborate with the NLP Sandbox
Team and its community to identify additional Tasks and then enable Developers
to benchmark the performance of tools that address these tasks. Examples of
additional tasks that may be added to the NLP Sandbox are the annotation of
medication intake, obesity status, and heart disease risk factor.

## Benchmarking NLP Tools

The NLP Sandbox has been designed to promote the development of Tools that have
the following properties:

- Robust and reliable
- Reproducible
- Re-usable
- Portable
- Cloud-friendly

A core element of the design of the NLP Sandbox in enabling the above properties
is the definition of common tool and data schemas. These schemas are defined
using the [OpenAPI specification] and are stored in the GitHub repository
[nlpsandbox/nlpsandbox-schemas] (see Section OpenAPI TODO). After collaborating
with an NLP Sandbox Driver to identify the specification of a new NLP Task /
Tool, we will make the new specification available to the community so that NLP
Developers can start building tools that meet the new specification.

In order to benchmark the performance of new NLP Tools, the Driver who is
proposing a new NLP Task should ideally also enable Developers to benchmark the
performance of their implementation by providing a dataset formatted according
to the proposed schemas. In the [model-to-data approach] implemented by the NLP
Sandbox, developers submit NLP Tools for evaluation on [nlpsandbox.io]. The
submitted tools are then automatically and concurrently pulled by NLP Sandbox
Data Hosting Sites that evaluate the performance of the tools on public or
private data. Because private datesets may include sensitive information such as
PHI, the evaluation of a tool is performed in a secure environment deployed and
controlled by the data provider (e.g. a university or hospital). The only
information returned by the Data Hosting Sites to the [nlpsandbox.io] is the
performance of the tools. This performance is then made visible to the community
in the Leaderboard defined for this NLP Task.

## Developing modular NLP Tools

One objective of the NLP Sandbox is to promote and accelerate the development of
tools that are modular and re-usable. This is achieved by decomposing complex
tasks into smaller tasks that can be individually benchmarked. As an example,
the [NLP Sandbox PHI Deidentifier Task][phi-annotation-task] proposed by CD2H
has been decomposed into the following annotation tasks (and counting):

- [Date Annotator][phi-annotation-task]
- [Person Name Annotator][phi-annotation-task]
- [Physical Address Annotator][phi-annotation-task]

The first benefit of identifying this modularity is that Tools become re-usable
buidling blocks that can be assembled differently to solve more than one tasks.
Here, we can envision that the Date Annotators and Person Name Annotators could
contribute to accelerate the development of tools for medication intake
annotation and identify "who took what drug at what time" in clinical notes.

Because the scope of these tasks is smaller than the entire de-identification
tasks, Developers are able to make their first submission to [nlpsandbox.io]
faster than if they were required to submit a complete PHI de-identification
tool.

Another motivation for benchmarking smaller tasks individually is to enable NLP
Developers to identify where their time and contributions can be best put to
use. For example, a Developer wants to contribute to the de-identification of
PHI in clinical notes. The Developer can use the NLP Sandbox Leaderboards to
identify that the there are already several Date Annotators that achieve near
perfect scores while no satisfying Physical Address Annotator has been submitted
yet.

<!-- Links -->

[National Center for Date to Health (CD2H)]: https://cd2h.org/
[nlpsandbox/nlpsandbox-schemas]: https://github.com/nlpsandbox/nlpsandbox-schemas
[OpenAPI specification]: https://github.com/OAI/OpenAPI-Specification
[nlpsandbox.io]: https://nlpsandbox.io
[NLP Sandbox PHI Deidentifier API]: https://nlpsandbox.github.io/nlpsandbox-schemas/phi-deidentifier/latest/docs
[phi-annotation-task]: https://www.synapse.org/#!Synapse:syn22277124/wiki/608037
[model-to-data approach]: https://doi.org/10.1186/s13059-019-1794-0
