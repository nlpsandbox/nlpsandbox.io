<!-- markdownlint-disable-next-line first-line-h1 -->
{row}
{column width=9}

### Introduction

The NLP Sandbox aims to provide a cloud-based environment that enables the systematic and continuous benchmarking of NLP Sandbox tools that solve specified tasks.

The first series of NLP Sandbox tasks targeted by the NLP Sandbox are the annotation and de-identification of Protected Health Information (PHI) in clinical notes. This first series of tasks have been identified through our collaboration with the [National Center for Date to Health (CD2H)].

However, we are counting on more organizations to work with the NLP Sandbox team and its community to identify additional tasks and then enable participants to submit and benchmark tools that address these tasks. Examples of additional tasks that may be added to the NLP Sandbox are the annotation of medication intake, obesity status, and heart disease risk factor.

### Benchmarking NLP Sandbox Tools

The NLP Sandbox has been designed to promote the development of tools that have the following properties:

- Robust and reliable
- Reproducible
- Re-usable
- Portable
- Cloud-friendly

A core element of the design of the NLP Sandbox in enabling the above properties is the definition of common tool and data schemas. These schemas are defined using the [OpenAPI specification] and are stored in the GitHub repository [nlpsandbox/nlpsandbox-schemas]. After working with an NLP Sandbox driver to identify the specification of a new NLP Sandbox task / tool, we will make the new specification available to the community so that NLP Sandbox participants can start building tools that meet the new specification.

In order to benchmark the performance of new NLP Sandbox tools, the organization who is proposing a new NLP Sandbox task should ideally also enable participants to benchmark the performance of their implementation by providing a dataset formatted according to the proposed schemas. In the [model-to-data approach] implemented by the NLP Sandbox, participants submit NLP Sandbox tools for evaluation on [NLPSandbox.io]. The submitted tools are then automatically and concurrently pulled by NLP Sandbox data hosting sites that evaluate the performance of the tools on public or private data. Because private datesets may include sensitive information such as PHI, the evaluation of a tool is performed in a secure environment deployed and controlled by the data provider (e.g. a university or hospital). The only information returned by the data hosting Sites to the [NLPSandbox.io] is the performance of the tools. This performance is then made visible to the community in the leaderboard defined for this NLP Sandbox task.

### Developing modular NLP Sandbox Tools

One objective of the NLP Sandbox is to promote and accelerate the development of tools that are modular and re-usable. This is achieved by decomposing complex tasks into smaller tasks that can be individually benchmarked. As an example, the [NLP Sandbox PHI Deidentifier Task][phi-annotation-task] proposed by CD2H has been decomposed into the following annotation tasks (and counting):

- [Date Annotator][phi-annotation-task]
- [Person Name Annotator][phi-annotation-task]
- [Physical Address Annotator][phi-annotation-task]

The first benefit of NLP Sandbox tasks' modularity is that the submitted tools are re-usable building blocks that can be re-combined to solve more than one task. Here, we can envision that the Date Annotators and Person Name Annotators could contribute to accelerate the development of tools for medication intake annotation and identify "who took what drug at what time" in clinical notes.

Moreover, because the scope of these tasks is smaller than the comprehensive de-identification tasks, participants are able to make their first submission to [NLPSandbox.io] faster than if they were required to submit a complete PHI de-identification tool.

Another motivation for benchmarking smaller tasks individually is to enable NLP Sandbox participants to identify where their time and contributions can be best put to use. For example, a participant wants to contribute to the de-identification of PHI in clinical notes. The participant can use the NLP Sandbox Leaderboards to identify that the there are already several Date Annotators that achieve near perfect scores while no satisfying Physical Address Annotator has been submitted yet.

{column}
{row}

<!-- Links -->

[National Center for Date to Health (CD2H)]: https://cd2h.org/
[nlpsandbox/nlpsandbox-schemas]: https://github.com/nlpsandbox/nlpsandbox-schemas
[OpenAPI specification]: https://github.com/OAI/OpenAPI-Specification
[NLPSandbox.io]: https://nlpsandbox.io
[NLP Sandbox PHI Deidentifier API]: https://nlpsandbox.github.io/nlpsandbox-schemas/phi-deidentifier/latest/docs
[phi-annotation-task]: https://www.synapse.org/#!Synapse:syn22277124/wiki/608037
[model-to-data approach]: https://doi.org/10.1186/s13059-019-1794-0
